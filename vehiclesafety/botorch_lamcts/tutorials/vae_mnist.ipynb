{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VAE MNIST example: BO in a latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this tutorial, we use the MNIST dataset and some standard PyTorch examples to show a synthetic problem where the input to the objective function is a `28 x 28` image. The main idea is to train a [variational auto-encoder (VAE)](https://arxiv.org/abs/1312.6114) on the MNIST dataset and run Bayesian Optimization in the latent space. We also refer readers to [this tutorial](http://krasserm.github.io/2018/04/07/latent-space-optimization/), which discusses [the method](https://arxiv.org/abs/1610.02415) of jointly training a VAE with a predictor (e.g., classifier), and shows a similar tutorial for the MNIST setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets # transforms\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.double"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problem setup\n",
        "\n",
        "Let's first define our synthetic expensive-to-evaluate objective function. We assume that it takes the following form:\n",
        "\n",
        "$$\\text{image} \\longrightarrow \\text{image classifier} \\longrightarrow \\text{scoring function} \n",
        "\\longrightarrow \\text{score}.$$\n",
        "\n",
        "The classifier is a convolutional neural network (CNN) trained using the architecture of the [PyTorch CNN example](https://github.com/pytorch/examples/tree/master/mnist)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We next instantiate the CNN for digit recognition and load a pre-trained model.\n",
        "\n",
        "Here, you may have to change `PRETRAINED_LOCATION` to the location of the `pretrained_models` folder on your machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "PRETRAINED_LOCATION = \"./pretrained_models\"\n",
        "\n",
        "cnn_model = Net().to(dtype=dtype, device=device)\n",
        "cnn_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_cnn.pt\"), map_location=device)\n",
        "cnn_model.load_state_dict(cnn_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our VAE model follows the [PyTorch VAE example](https://github.com/pytorch/examples/tree/master/vae), except that we use the same data transform from the CNN tutorial for consistency. We then instantiate the model and again load a pre-trained model. To train these models, we refer readers to the PyTorch Github repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc21 = nn.Linear(400, 20)\n",
        "        self.fc22 = nn.Linear(400, 20)\n",
        "        self.fc3 = nn.Linear(20, 400)\n",
        "        self.fc4 = nn.Linear(400, 784)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = F.relu(self.fc1(x))\n",
        "        return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = F.relu(self.fc3(z))\n",
        "        return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "vae_model = VAE().to(dtype=dtype, device=device)\n",
        "vae_state_dict = torch.load(os.path.join(PRETRAINED_LOCATION, \"mnist_vae.pt\"), map_location=device)\n",
        "vae_model.load_state_dict(vae_state_dict);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now define the scoring function that maps digits to scores. The function below prefers the digit '3'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def score(y):\n",
        "    \"\"\"Returns a 'score' for each digit from 0 to 9. It is modeled as a squared exponential\n",
        "    centered at the digit '3'.\n",
        "    \"\"\"\n",
        "    return torch.exp(-2 * (y - 3)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the scoring function, we can now write our overall objective, which as discussed above, starts with an image and outputs a score. Let's say the objective computes the expected score given the probabilities from the classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def score_image_recognition(x):\n",
        "    \"\"\"The input x is an image and an expected score based on the CNN classifier and\n",
        "    the scoring function is returned.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        probs = torch.exp(cnn_model(x))  # b x 10\n",
        "        scores = score(torch.arange(10, device=device, dtype=dtype)).expand(probs.shape)\n",
        "    return (probs * scores).sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define a helper function `decode` that takes as input the parameters `mu` and `logvar` of the variational distribution and performs reparameterization and the decoding. We use batched Bayesian optimization to search over the parameters `mu` and `logvar`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def decode(train_x):\n",
        "    with torch.no_grad():\n",
        "        decoded = vae_model.decode(train_x)\n",
        "    return decoded.view(train_x.shape[0], 1, 28, 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model initialization and initial random batch\n",
        "\n",
        "We use a `SingleTaskGP` to model the score of an image generated by a latent representation. The model is initialized with points drawn from $[-6, 6]^{20}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from botorch.models import SingleTaskGP\n",
        "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
        "from botorch.utils.transforms import standardize, normalize, unnormalize\n",
        "\n",
        "d = 20\n",
        "bounds = torch.tensor([[-6.0] * d, [6.0] * d], device=device, dtype=dtype)\n",
        "\n",
        "\n",
        "def gen_initial_data(n=5):\n",
        "    # generate training data  \n",
        "    train_x = unnormalize(torch.rand(n, d, device=device, dtype=dtype), bounds=bounds)\n",
        "    train_obj = score_image_recognition(decode(train_x)).unsqueeze(-1)  # add output dimension\n",
        "    best_observed_value = train_obj.max().item()\n",
        "    return train_x, train_obj, best_observed_value\n",
        "\n",
        "def get_fitted_model(train_x, train_obj, state_dict=None):\n",
        "    # initialize and fit model\n",
        "    model = SingleTaskGP(train_X=train_x, train_Y=train_obj)\n",
        "    if state_dict is not None:\n",
        "        model.load_state_dict(state_dict)\n",
        "    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
        "    mll.to(train_x)\n",
        "    fit_gpytorch_model(mll)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Define a helper function that performs the essential BO step\n",
        "The helper function below takes an acquisition function as an argument, optimizes it, and returns the batch $\\{x_1, x_2, \\ldots x_q\\}$ along with the observed function values. For this example, we'll use a small batch of $q=3$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from botorch.optim import optimize_acqf\n",
        "\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "\n",
        "def optimize_acqf_and_get_observation(acq_func):\n",
        "    \"\"\"Optimizes the acquisition function, and returns a new candidate and a noisy observation\"\"\"\n",
        "    \n",
        "    # optimize\n",
        "    candidates, _ = optimize_acqf(\n",
        "        acq_function=acq_func,\n",
        "        bounds=torch.stack([\n",
        "            torch.zeros(d, dtype=dtype, device=device), \n",
        "            torch.ones(d, dtype=dtype, device=device),\n",
        "        ]),\n",
        "        q=BATCH_SIZE,\n",
        "        num_restarts=10,\n",
        "        raw_samples=200,\n",
        "    )\n",
        "\n",
        "    # observe new values \n",
        "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
        "    new_obj = score_image_recognition(decode(new_x)).unsqueeze(-1)  # add output dimension\n",
        "    return new_x, new_obj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Perform Bayesian Optimization loop with qEI\n",
        "The Bayesian optimization \"loop\" for a batch size of $q$ simply iterates the following steps: (1) given a surrogate model, choose a batch of points $\\{x_1, x_2, \\ldots x_q\\}$, (2) observe $f(x)$ for each $x$ in the batch, and (3) update the surrogate model. We run `N_BATCH=75` iterations. The acquisition function is approximated using `MC_SAMPLES=2000` samples. We also initialize the model with 5 randomly drawn points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from botorch import fit_gpytorch_model\n",
        "from botorch.acquisition.monte_carlo import qExpectedImprovement\n",
        "from botorch.sampling.samplers import SobolQMCNormalSampler\n",
        "\n",
        "seed=1\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "N_BATCH = 50\n",
        "MC_SAMPLES = 2000\n",
        "best_observed = []\n",
        "\n",
        "# call helper function to initialize model\n",
        "train_x, train_obj, best_value = gen_initial_data(n=5)\n",
        "best_observed.append(best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to run the BO loop (this make take a few minutes, depending on your machine)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running BO .................................................."
          ]
        }
      ],
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(f\"\\nRunning BO \", end='')\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "state_dict = None\n",
        "# run N_BATCH rounds of BayesOpt after the initial random batch\n",
        "for iteration in range(N_BATCH):    \n",
        "\n",
        "    # fit the model\n",
        "    model = get_fitted_model(\n",
        "        normalize(train_x, bounds=bounds), \n",
        "        standardize(train_obj), \n",
        "        state_dict=state_dict,\n",
        "    )\n",
        "    \n",
        "    # define the qNEI acquisition module using a QMC sampler\n",
        "    qmc_sampler = SobolQMCNormalSampler(num_samples=MC_SAMPLES, seed=seed)\n",
        "    qEI = qExpectedImprovement(model=model, sampler=qmc_sampler, best_f=standardize(train_obj).max())\n",
        "\n",
        "    # optimize and get new observation\n",
        "    new_x, new_obj = optimize_acqf_and_get_observation(qEI)\n",
        "\n",
        "    # update training points\n",
        "    train_x = torch.cat((train_x, new_x))\n",
        "    train_obj = torch.cat((train_obj, new_obj))\n",
        "\n",
        "    # update progress\n",
        "    best_value = train_obj.max().item()\n",
        "    best_observed.append(best_value)\n",
        "    \n",
        "    state_dict = model.state_dict()\n",
        "    \n",
        "    print(\".\", end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EI recommends the best point observed so far. We can visualize what the images corresponding to recommended points *would have* been if the BO process ended at various times. Here, we show the progress of the algorithm by examining the images at 0%, 10%, 25%, 50%, 75%, and 100% completion. The first image is the best image found through the initial random batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAywAAACQCAYAAADqbu8OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAFTlJREFUeJzt3XuMVOX9x/HPsGhQFCXgraYJiekf\nXv6wqSuLukhdKf7wZ5os65FYtdYDP4hNqH+sYDReg1GsDQ2C9cKqlf6hT4jxklStYCZE0WXZqInR\nhGgCmmpjEy/Iiiuw+/uDGdnn+Mz9XJ6Zeb/+cb5zOee7Zz7M5PGcZ57c+Pi4AAAAAMBHk7JuAAAA\nAABKYcACAAAAwFsMWAAAAAB4iwELAAAAAG8xYAEAAADgLQYsAAAAALw1ud4XBkGwVlKXpHFJfzLG\nDMXbGpoRuYALuYALuYALuYALuWhvdZ1hCYLgYkm/MMbMkRRKWhd/a2g25AIu5AIu5AIu5AIu5AL1\nnmHpkfS8JBljPgyCYHoQBNOMMXujT9y6dSsrUza5np6eXLVPJRftg1zAhVzAhVzAhVzAxZWLegcs\np0oanlD/t3DfT4IjSatWrVIYhhoYGKhzd8mjP7c1a9bU8nRykTJykQ36cyMX9OdCLujPhVzQn0up\nXNQ7YImOfHKFawqdwjDUzJkzFYZhnbtLHv3FglykzPf+CshFynzvr4BcpMz3/grIRcp876+AXKTM\nt/7qHbD8uzCyLfqZpP+UevLAwAAjyQb5NtItgVykjFxkg/7cyAX9uZAL+nMhF/TnUioX9f6s8b8k\n9enwRKhfSvrMGPNtIw2iJZALuJALuJALuJALuJCLNlfXgMUYs13ScBAE2yU9JOmP8beGZkMu4EIu\n4EIu4EIu4EIuUPc6LMaYW+JtBa2AXMCFXMCFXMCFXMCFXLQ3VroHAAAA4C0GLAAAAAC8xYAFAAAA\ngLcYsAAAAADwFgMWAAAAAN5iwAIAAADAWwxYAAAAAHiLAQsAAAAAbzFgAQAAAOCtule6x2HLli2z\n6qVLlza0vSeeeMKqH3744Ya2Bz+89dZbVn3UUUc1tL3zzjuvwY4AAD7J5XJl67GxsZQ7gg/IxWGc\nYQEAAADgLQYsAAAAALzFgAUAAACAt5jDUsHOnTtT3d8NN9xg1cxhaQ5p56S4v3w+r507dzKnpUpx\nvU/F496om266yarfeOONml5fqodq+yM3h/mWi0bxvsaj2XMRhqFVv/fee6n30IrIRTY4wwIAAADA\nWwxYAAAAAHiLAQsAAAAAbzGHJcKH648nYq6CH3zLBarj+/u2a9euso+vWbPGqnt6emLd/9DQkFV3\ndnbGun1f+Z6LRkX/Pr43qtNquRgYGCj7OLmoDrnwA2dYAAAAAHiLAQsAAAAAbzFgAQAAAOCttp/D\nsmPHDknStm3bfrwNkAvEodFrgVetWmXVcV9LncvlYt1eq4rO7RkfH3c+LwxD9ff3t9w173Cr9t83\nuWgv5CIZnGEBAAAA4C0GLAAAAAC8xYAFAAAAgLfafg7LpEmTnLerVelaxbffftuqJ09u+0PeFBrN\nxdjYmFWff/75Vs01q8mo571qhK+/Vw9bpX9vcb+Plba3du1aq3733Xeturu726rPPffcGLtDUfT7\nOSrtXJx00klWvX//fqueP3++Vd9222017T/6eriRCz9xhgUAAACAtxiwAAAAAPAWAxYAAAAA3mJC\nRQXDw8NWvWzZsppe/9VXX1l19FpEtKbonBWkIzp36IILLrDq7du317S9jz76yKoXL14sTfj9/GbX\nLnNwfPs7o3NUovW+ffusOvo9Mn369LLb9+3v9VVXV1fWLVhefvnlso/v2bPHqvfu3WvV06ZNK/v6\naI7gRi78xBkWAAAAAN6q6gxLEATnSHpB0lpjzPogCH4uaZOkDkmfS7rWGDOafLvwCbmAC7mAC7mA\nC7mAC7lAVMUzLEEQTJX0kKStE+6+R9IGY0y3pI8k3ZBsm/ANuYALuYALuYALuYALuYBLNWdYRiUt\nlLRqwn3zJC0v3H5JUr+kvyXUY6K+++47qXDt+3fffae5c+fGuv0WnrPS0rlYsWKFJKmzs1MrVqzQ\nunXrrMcPHTpk1bNnzy67vSVLlsTa39VXXy0Vfj/9sccei3XbDfIqFz/88INVt/u1/Rn+/V7lIm21\nrrv05ZdfWnVvb2/MHXmDXNTglFNOseqLLroo5o68QS5q0C65qDhgMcYclHQwCIKJd0+dcCruC0mn\nJdYhvEQu4EIu4EIu4EIu4EIu4FLvr4SNT7idi9Q/EYahZs6cqTAM69xdcoormo6MjOjtt9+Ovcd8\nPh/Ldvbt26d8Pu/lMZygZXJx3HHHSZKmTp2qzs7Oiu9jpb9hxowZVt1oLoor006bNk3z58//ya8M\neaZlclGUVX/V5qb4eRHl2TFtm1zU+u89emYwrr/Z9+NXQC5KGB+3DwW5KI1cNM6341fvgGUkCIJj\njDH7JZ1emABV0sDAgMIw1MDAQJ27S862bdukwsClq6tLK1eujHX7tZ7aKyWfz2vevHmp/5TqmjVr\nanl6y+Si+HO4nZ2dGhoa0rXXXms9Hr0krNL7Er0k7Morr2yov+JlYPPnz9drr72mXbt2NbS9WrVr\nLoqy6q/az5Pi50VU0p8f5MLdX63fA5988olV33rrrQ33pgxzSy7iycX3339v1TfffHPDvYlcZIZc\nuJXKRb0Dli2SFkn6R+G/rzTUXYaKc1bCMIx9sCLHyDeXy9X0+uI1502y7kPL5KK4XseZZ56p7du3\nN3zt//Lly6t4VvWKA5Tu7u7UByt1aJlcpO2EE05o6PWerwdELkpo4Tkr1SAXJbTq3IQqkYsS2iUX\nFQcsQRD8StJfJM2SdCAIgj5Jv5P0VBAEyyTtkfT3dNqFL8gFXMgFXMgFXMgFXMgFXKqZdD9c+HWG\nqPnJtIRmQC7gQi7gQi7gQi7gQi7gwkr3AAAAALxV7xwWVKmzs9OqBwcHrbqjo8Oqi5O90dzi+rGF\nonZfP8RXb7zxhlVPmTLFqht937Zu3VrFs4545513JEn79+/XO++8o7GxsYb2j/oUf8ylWvz7bg+V\n1uuKIhftgVxUhzMsAAAAALzFgAUAAACAtxiwAAAAAPBWy89hqTSXIO1rAWu9VhHZmDTp8Fg+l8tp\n0qRJFecCbNq0Kdb9RxemhJ+iK5JH57BE112KrstUyYEDB6z6qKOOKvv8pUuXSoV1m26//faa9oX6\ndXd3S5KOP/54dXd369hjj826JXhow4YNWbcAD5GL6nCGBQAAAIC3GLAAAAAA8BYDFgAAAADeark5\nLLWuf1F8fj6f186dO/XMM89Yjz/44IOx9odsROcWRNfPKCWfz2vHjh0N7/+pp56y6iAIrDp6zTvr\nZzSHSy65JNbt3XvvvVZdac5KV1dXrPtHdUp9z+TzeV1zzTWJ7z+aizfffNOqi3Pwoorfc+26jkPS\nyuWinrW5ent7rfq5554r+/zoXMozzzyz7PPvv/9+SdL06dPV19enzZs319wjKiMX8eAMCwAAAABv\nMWABAAAA4C0GLAAAAAC81fRzWOq5/q+cxYsXl62j6yh0dnbGun/E4+ijj7bqauesxCW6jsr69evL\n1tEcR9fvQGvo6Oiw6sHBwYa2d/DgwQY7gsuCBQusOjq3KG5xf4/Vur8VK1ZY9fbt21Ptp1lEP5eH\nhoYS3d+tt95atm7ULbfcIhXmUixevPjHuoi5TtUhF+ngDAsAAAAAbzFgAQAAAOAtBiwAAAAAvNV0\nc1jy+Xym+1+9enWi24/+fn5xDZDi73U/8sgj1uMbN25MtJ9mlfU12LNnz67p+ZWuCZ01a5ZVP/ro\no5Kkd999V6+++upPrrmHnxqdsxJ1xhlnWPXHH38c6/bbVdJzVnyzbt06q2buglvScxN8E53rRC7c\nyEU6ueAMCwAAAABvMWABAAAA4C0GLAAAAAC81XRzWI477rhU9/fBBx9Y9QsvvJDo/opzVkpZvny5\nVV9//fVWfdFFFyXSl+/OPvvsTPef9DWcmzdvdt7f0dGhGTNm/OSa0iuuuMKqP//880T7g9vdd9+d\n6PafffZZq+Ya8/qcfPLJZR//5ptvrPryyy+36u+//16SFIah+vv7a95/dI7a7t27a3p92uu4tIvo\nnNLoukcXXnihVUfX3yqqNxcXXHCBVQ8PD1v16Oho2deTi2SQi2xwhgUAAACAtxiwAAAAAPAWAxYA\nAAAA3mq6OSxJGxkZserrrrsus15QvaTnCkT5PlcgOrfpvvvuy6yXdhZdt+nOO+8s+/xmvba42c2Y\nMcOq58yZY9UHDhxIdP+1zllBOnK5nFV3dXWluv+s1xODG7nIBmdYAAAAAHiLAQsAAAAAbzFgAQAA\nAOCtppvDMm/ePKvO5/Oxbv/iiy+OdXuVNHrN+pQpU2LrpZn19fVZddxzAdKes9Jo/4sWLbJq5rBk\no9a5D7Nnz7bqwcHBmDuCy4cffph1C/BQqfUz0N7IRTY4wwIAAADAW1WdYQmC4AFJ3YXn3ydpSNIm\nSR2SPpd0rTGm/NKaaDnkAi7kAi7kAlFkAi7kAi4Vz7AEQfBrSecYY+ZIukzSXyXdI2mDMaZb0keS\nbkinXfiCXMCFXMCFXCCKTMCFXKCUas6wbJO0o3D7K0lTJc2TtLxw30uS+iX9LcE+f7Rv375Yt7dw\n4UJJUm9vrx544IFYt+3SQusseJWLqK+//tqqTzzxxCzaqBq5gFr72mhyEaMW+bwgEzEjF3BpkVxU\nHrAYYw5JKq6muETSPyUtmHA67gtJpyXbJnxDLuBCLuBCLhBFJuBCLlBK1b8SFgTBbyWFkn4jadeE\nh3KSxsu9NgxDzZw5U2EYNtSsS6O/Etbb2ysV/g98b29v4isa19vvvn37qnptEse4HF9zMTw8bNUd\nHR11bad43JM+ruTiiCRzEZe0+qs1F8WefD1+5CIerfR50UgmRC4s5OIIcnFEq+Si2kn3CyTdJuky\nY8w3QRCMBEFwjDFmv6TTC5OgShoYGFAYhhoYGIit8aJGT3UVLwPr7e3Vc889py+++CKmztzq7Tef\nz//kJ51d+vv769p+KWvWrCn5mM+52LJli1XXe0lY8bjHfVyjyMURSeYiLmn1V2suiu9zVsePXPiZ\niyLfPi8azYTIhYVcHEEujmiVXFQcsARBcIKkP0u61BjzZeHuLZIWSfpH4b+vxNptDaLrY0yaZP+O\nwI4dO6z6qquusuriAOXAgQOJDFbKfYHHIe31QYp8z8Wll15a9vFXX33Vql988UWr3rBhg1T4QEli\nsJL0NaUrV65MdPul+J4L3zWai1wuZ90eH6/4PyJTQS4QRSaaz8TPF0mJfL6Qi+aTRi5U5RmWqyTN\nlGSCICje93tJG4MgWCZpj6S/J9IdfEYu4EIu4EIuEEUm4EIu4FTNpPvHJD3meGh+Mi2hGZALuJAL\nuJALRJEJuJALlMJK9wAAAAC8VfWvhDWLsbExq057jkd0Dk1PT0+q+0d1FixYkOr+0v4d9Ndffz3V\n/fni4Ycftuobb7wxs16qEXcuBgcHJUnbtm3T4OCgzj///Fi336qi3xPr16+36smTq/uqzOfz2rlz\np6677jrr/ieffNKq6/3VQqRr2rRpVn3HHXdYdVdXl1VHP3c//fRTSdKMGTO0dOlSPf7449bjZ511\nllVfcsklVn3ZZZdZ9amnnlrz34D4kYtscIYFAAAAgLcYsAAAAADwFgMWAAAAAN5quTksWYuu+xK3\nrNZdQXmV1v9JGrk4LO05K5s3b7bqWbNmpbr/qIk5jGYSpQ0PD1t1o3NMnn766QY7ildfX58kaeHC\nhVq/fr12796ddUtNYe/evVYdnRM2ZcoUq164cKFzO/l8Xn19fVq2bFkCXdZv9erVkqQzzjhDq1ev\n1vPPP591S02BXGSDbzQAAAAA3mLAAgAAAMBbDFgAAAAAeIs5LDGLziWIXgtdXCehlOg6MsVrI8Mw\nVH9/f2x9Il6V1v+JrsuwYsWKhvZX3D65yNb7779v1VnPYRkZGZEKeSzeRmXj4+NWHV33YMuWLVad\n9Doqhw4dsurZs2fHst3R0VHmrzRg7ty5Vr1kyRKrXr58eaz7i36vdHd3W/Xo6Ggs+wnD0Jt5Cs2I\nXKSDMywAAAAAvMWABQAAAIC3GLAAAAAA8BZzWBIWvRaZ9TLaU3RdBt/WaUB97rrrrrJ1VsIw1MqV\nK7Nuo2l9++23Vl3tHBLmlLWXjRs3lq2LyEV7IRfJ4AwLAAAAAG8xYAEAAADgLQYsAAAAALzFgAUA\nAACAtxiwAAAAAPAWAxYAAAAA3mLAAgAAAMBbDFgAAAAAeIsBCwAAAABvMWABAAAA4C0GLAAAAAC8\nlRsfH090B1u3bk12B0hcT09PLu5tkovmRy7gQi7gQi7gQi7g4spF4gMWAAAAAKgXl4QBAAAA8BYD\nFgAAAADeYsACAAAAwFsMWAAAAAB4a3IaOwmCYK2kLknjkv5kjBlKY78VejpH0guS1hpj1gdB8HNJ\nmyR1SPpc0rXGmNEM+3tAUnfhPbpP0pBP/cWBXNTVH7nIpidykTFyUVd/5CKbnshFxshFXf15nYvE\nz7AEQXCxpF8YY+ZICiWtS3qfVfQ0VdJDkrZOuPseSRuMMd2SPpJ0Q4b9/VrSOYVjdpmkv/rUXxzI\nRV39kYtseiIXGSMXdfVHLrLpiVxkjFzU1Z/3uUjjkrAeSc9LkjHmQ0nTgyCYlsJ+yxmVtFDSZxPu\nmyfpxcLtlyRdmlFvkrRN0pWF219JmupZf3EgF7UjF9kgF9kjF7UjF9kgF9kjF7XzPhdpXBJ2qqTh\nCfV/C/ftTWHfTsaYg5IOBkEw8e6pE051fSHptGy6k4wxhySNFMolkv4paYEv/cWEXNSIXGSDXHiB\nXNSIXGSDXHiBXNSoGXKRxoAlulplrnBNoW8m9uRFj0EQ/LZwOvM3knZNeMiL/hpELupELrxALtJF\nLupELrxALtJFLurkcy7SuCTs34WRbdHPJP0nhf3WaiQIgmMKt08vTDDKTBAECyTdJul/jDHf+NZf\nDMhFHciFN7w67uTCG14dd3LhDa+OO7nwhlfH3fdcpDFg+ZekPh0+GL+U9Jkx5tsU9lurLZIWFW4v\nkvRKVo0EQXCCpD9L+l9jzJe+9RcTclEjcuEVb447ufCKN8edXHjFm+NOLrzizXFvhlzkxseTP8MT\nBMH9kuZKGpP0R2PMe4nvtHw/v5L0F0mzJB0ojMZ/J+kpSVMk7ZH0B2PMgYz6+z9Jd0VOx/1e0kYf\n+osLuai5P3KRTT/kwgPkoub+yEU2/ZALD5CLmvvzPhepDFgAAAAAoB6sdA8AAADAWwxYAAAAAHiL\nAQsAAAAAbzFgAQAAAOAtBiwAAAAAvMWABQAAAIC3GLAAAAAA8BYDFgAAAADe+n/j9BlBmAHiHwAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1008x1008 with 6 Axes>"
            ]
          },
          "metadata": {
            "bento_obj_id": "140558410135648"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(1, 6, figsize=(14, 14))\n",
        "percentages = np.array([0, 10, 25, 50, 75, 100], dtype=np.float32)\n",
        "inds = (N_BATCH * BATCH_SIZE * percentages / 100 + 4).astype(int)\n",
        "\n",
        "for i, ax in enumerate(ax.flat):\n",
        "    b = torch.argmax(score_image_recognition(decode(train_x[:inds[i],:])), dim=0)\n",
        "    img = decode(train_x[b].view(1, -1)).squeeze().cpu()\n",
        "    ax.imshow(img, alpha=0.8, cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
